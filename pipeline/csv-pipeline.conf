input {
  file {
    path => "/data/uploads/*.csv"
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/data/sincedb_csv"
    mode => "read"
    file_completed_action => "log"
    file_completed_log_path => "/usr/share/logstash/data/completed_csv.log"
    codec => plain
  }
}

filter {
  # Parser le CSV avec headers définis
  csv {
    separator => ","
    columns => ["timestamp", "transaction_id", "customer_id", "amount", "payment_type", "status", "country", "product_category", "error_message"]
    skip_header => true
  }
  
  # Parser le champ timestamp si présent
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss", "yyyy-MM-dd'T'HH:mm:ss'Z'", "yyyy-MM-dd'T'HH:mm:ssZ"]
      target => "@timestamp"
      remove_field => ["timestamp"]
    }
  }
  
  # Ajouter des métadonnées sur le fichier source
  mutate {
    add_field => {
      "source_type" => "csv"
      "source_file" => "%{[log][file][path]}"
      "ingestion_timestamp" => "%{@timestamp}"
    }
  }
  
  # Normaliser le champ level si présent (uppercase)
  if [level] {
    mutate {
      uppercase => ["level"]
    }
  }
  
  # Supprimer les champs Logstash internes non nécessaires
  mutate {
    remove_field => ["host", "event"]
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "logstash-csv-%{+YYYY.MM.dd}"
    data_stream => false
  }
  
  # Debug: afficher dans stdout
  stdout {
    codec => rubydebug {
      metadata => true
    }
  }
}
